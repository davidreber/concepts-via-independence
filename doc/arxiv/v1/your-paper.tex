\pdfoutput=1
\documentclass{article}


% alptex + small aesthetic tweaks + some extra math defs
\input{preamble/preamble.tex}
\input{preamble/preamble_math}
\input{preamble/definitions_basic}

% bibtex import + some code to strip away useless bib info (volume number, isbn, and the ilk), and to standardize capitalization
% warning: the arxiv uses an outdated bibtex, which causes cryptic and frustrating upload errors 
% easiest solution: install whatever current arxiv texlive is from ftp://tug.org/historic/systems/texlive/ (download the ISO) and compile using this versions pdflatex and bibtex
% alternatively, look upon https://github.com/plk/biblatex/wiki/biblatex-and-the-arXiv and despair. 
\input{preamble/minimalist_biblatex}

\addbibresource{bibs/sensitivity}

\crefformat{equation}{(#2#1#3)}
\crefformat{figure}{Figure~#2#1#3}
\crefname{example}{Example}{Examples}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{cor}{Corollary}{Corollaries}
\crefname{theorem}{Theorem}{Theorems}
\crefname{assumption}{Assumption}{Assumptions}

%********************************************************************
% Extra definitions
%********************************************************************
\usepackage{enumitem} % tight enumerates
\usepackage[separate-uncertainty=true,multi-part-units=single]{siunitx} % better table control

\newcommand{\maxf}[1]{{\cellcolor[gray]{0.8}} #1}
\global\long\def\embedding{\lambda}

% Peter's grey box
\declaretheoremstyle[
%    postheadspace=\newline,
spacebelow=\parsep,
    spaceabove=\parsep,
  mdframed={
    backgroundcolor=gray!10!white,     % vv: weird spacing issue, so leaving transpartent for now
    hidealllines=true, 
    innertopmargin=8pt, 
    innerbottommargin=4pt, 
    skipabove=8pt,
    skipbelow=10pt,
    nobreak=true
}
]{grayboxed}
\declaretheorem[style=grayboxed,name=Assumption]{gassumption}
% \declaretheorem[style=plain]{auxtheorem}
% \declaretheorem[style=grayboxed,sibling=auxtheorem]{algorithm}
% \declaretheorem[style=grayboxed,name=Algorithm]{nalgorithm}
\crefname{gassumption}{Assumption}{Assumptions}

\usepackage{thm-restate}

%********************************************************************
% Dan Roy's commenting code
%********************************************************************
\usepackage{xcolor}
\input{preamble/commenting.tex}
%\input{preamble/myvruler.tex}
%For submission, uncomment these lines to make all annotations render as blank.
% \renewcommand{\LATER}[1]{}
% \renewcommand{\fLATER}[1]{}
% \renewcommand{\TBD}[1]{}
% \renewcommand{\fTBD}[1]{}
% \renewcommand{\PROBLEM}[1]{}
% \renewcommand{\fPROBLEM}[1]{}
% \renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

% frontmatter
\usepackage[affil-it]{authblk}

\title{Searching for Model Concepts by their Conditional Independencies}
\date{}
\author[1]{David Reber}
% \author[1,2]{Victor Veitch}
% \affil[1]{Physical Insitute of the Beyond}
% \affil[2]{Ouija Statistical Institute}

\begin{document}
\maketitle

\begin{abstract}
Your abstract here
\end{abstract}

So far Victor Veitch and Kaarel HÃ¤nni have helped brainstorm and soundboard! ChatGPT probably helped a lot with formatting, rephrasing, and catching spelling mistakes. All remaining mistakes are my own.

\section{April 25 Victor summary}
\begin{itemize}
    \item Assume you have infinite compute and infinite data
    \item It seems like even then you may not be able to do this search easily, because there are infinite rotations you would need to check (there's no privileged basis).
    \item The counterpoint is that LLMs seem to be able to also have an understanding of which concepts are conditionally independent: if you assume that knowledge is also represented somewhere in the model, does that buy you the extra constraints you need to narrow down the search space to something that ever terminates?
\end{itemize}

\section{Rough Notes from April 25 discussion with Kaarel}
\begin{itemize}
    \item Transformer model
    \item Causal graph has 3 nodes (actually a CI)
    \item Try all possible variations of 3 places in the residual stream to look at
    \begin{itemize}
        \item Both token position and layer: all possible combinations of places
    \end{itemize}
    \item Look for a feature in each position, testing whether each feature has the independence you want
    \item Should be possible to do using a differentiable loss function
    \begin{itemize}
        \item If binary concepts are sigmoids of affine transformations of features
        \item Then loss depends on whether the CI holds in that batch
        \item KL divergence between product distribution and joint distribution you observe
        \item Need an empirical variant of the mutual information.
    \end{itemize}
    \item Gradient descent is on how to choose 3 positions in the model.
    \item If there are any other concepts which have the same causal structure, this won't be able to differentiate between them
    \begin{itemize}
        \item So we could add a supervised term which matches prompt = "man"
    \end{itemize}
    \item If the 3 variables have deterministic value from a given input, then we could train a supervised probe to match the labels I know to the positions in the model.
\end{itemize}

\end{document}

